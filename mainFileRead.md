# Detail
- [[Site Loader]]ë¥¼ í™œìš©í•˜ì—¬ `Site`ì˜ ì •ë³´ë¥¼ ê°€ì €ì™€ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì£¼ëŠ” ì¶œë ¥í•´ì£¼ëŠ” Pageì´ë‹¤.
- ê¸°ë³¸ UIëŠ” [[Document GPT]]ì™€ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤.
- LLMì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.
	- `History Model` : [[Stuff LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ Userì˜ `Question`ê³¼ `Histroy`(AIì™€ userì˜ Message)ë¥¼ ë°›ì€ ë’¤, í•´ë‹¹ `Question`ì´ ì˜ˆì „ì˜ ì¡´ì¬í•˜ì˜€ë‹¤ë©´ í•´ë‹¹ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³  ì¡´ì¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´ `None`ì„ ì¶œë ¥í•´ì£¼ëŠ” `Model`
	- `Research Model` : [[Map Re-rank LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ Retrieverì—ì„œ Userì˜ `Question`ì— ì•Œë§ì€ ë‹µë³€ì„ ì°¾ì•„ ì¶œë ¥í•´ì£¼ëŠ” `Model`
- `Memory`, `Message`, `Chain` ë“± ì—¬ëŸ¬ ê¸°ëŠ¥ë“¤ì„ ì‰½ê²Œ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê°ê°ì„ **package ë¶„í• **í•˜ì˜€ë‹¤.
	- `SiteGPT.py` : `SiteGPT`ì˜ ë©”ì¸ Pageë¥¼ UIì„ êµ¬ì„±í•˜ì—¬ ì¶œë ¥í•˜ëŠ” file
	- `Utils.py` : Userì™€ AIì˜ ëŒ€í™”ë¥¼ ì¶œë ¥í•˜ê³  ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
	- `Data_process.py` :  [[Site Loader]]ë¡œ ë°ì´í„°ë¥¼ ë°›ê±°ë‚˜ í•´ë‹¹ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
	- `Chain.py` : `LLM`, `Memory` ê´€ë ¨ëœ ëª¨ë“  ê¸°ëŠ¥ë“¤ì˜ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
# Code
#### SiteGPT.py
- [[Streamlit]]ì˜ `Side bar` Widgitì„ í™œìš©í•˜ì—¬ ìœ ì €ì—ê²Œ `URL`ì„ ë°›ê³  í•´ë‹¹ ê°’ì„ `Data_process.py`ì— ë„˜ê²¨ Retreiverì„ ë°›ëŠ”ë‹¤.
- Retreiver ê°’ì— ìœ ì €ì˜ ì§ˆë¬¸ì„ ë”í•˜ì—¬ `Chain.py` ë„˜ê²¨ `AI`ì˜ Responseì„ ë„˜ê²¨ ë°›ëŠ”ë‹¤. 
- ì´ëŸ¬í•œ ê³¼ì •ì„ ê±°ì³ ì–»ì€ Responseì„ `Utils.py`ì˜ functionì„ ì´ìš©í•´ Siteì— ì¶œë ¥í•œë‹¤.
- URLì˜ ê°’ì´ ì—†ì„ ë•Œ, `Memory` ê°’ê³¼ `Message`ì´ ì´ˆê¸°í™” ë˜ë„ë¡ êµ¬í˜„í•˜ì˜€ë‹¤.
```python
import streamlit as st
from pages.SiteGPT.utils import paint_message, send_message
from pages.SiteGPT.data_process import get_retriever_in_website
from pages.SiteGPT.chain import invoke_chain, initialize_memory

st.set_page_config(
    page_title="Site GPT",
    page_icon="ğŸ¤£",
)

st.title("Site GPT")

st.markdown(
    """
    Ask questions about the content of a website.

    Start by writing the URL of the website on the sidebar.
    """
)

# ex) https://deepmind.google/sitemap.xml

with st.sidebar:
    url = st.text_input(
        "Write down a URL",
        placeholder="https://example.com/sitemap.xml",
    )

if url:
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a Stiemap URL")
    else:
        retriever = get_retriever_in_website(url)
        send_message(st.session_state["messages"], "How can I help you?", "ai", save=False)
        paint_message(st.session_state["messages"])
        question = st.chat_input("Ask any questions in the document!")
        if question:
            send_message(st.session_state["messages"], question, "human")
            invoke_chain(st.session_state["messages"], retriever, question)
else:
    st.session_state["messages"] = []
    initialize_memory()
```
#### Utils.py
- `paint_message` : ê¸°ë¡ëœ ëª¨ë“  messagesì„ ì¶œë ¥í•œë‹¤.
- `save_message` : messageì™€ roleë¥¼ ì €ì¥í•œë‹¤.
- `send_message` : messageì„ ì¶œë ¥í•˜ê³  `Save` ì—¬ë¶€ì— ë”°ë¼ messageë¥¼ ì €ì¥í•œë‹¤.
```python
import streamlit as st

def paint_message(messages):
    for message in messages:
        send_message(messages, message["message"], message["role"], save=False)
        
def save_message(messages, message, role):
    messages.append({"message": message, "role": role})

def send_message(messages, message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
        if save:
            save_message(messages, message, role)
```
#### Data_process.py
- `parse_page` : `SitemapLoader`ì„ í†µí•´ ê°€ì ¸ì˜¨ `Data`ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.
- `get_retriever_in_website` : `st.cache_resource`ì„ ì‚¬ìš©í•˜ì—¬ URLì´ ë°”ë€” ë•Œë§Œ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ê³ , [[Retrieval]]ì˜ ì „ë°˜ì ì¸ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.
```python
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import SitemapLoader
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings

def parse_page(soup):
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return str(soup.get_text()).replace("\n", " ").replace("\xa0", " ")

@st.cache_resource(show_spinner="Loading website....")
def get_retriever_in_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )
    loader = SitemapLoader(
        url,
        parsing_function=parse_page,
    )
    loader.requests_per_second = 5
    docs = loader.load_and_split(text_splitter=splitter)
    url_name = (
        str(url).replace("https://", "").replace(".", "").replace("/sitemapxml", "")
    )
    cache_dir = LocalFileStore(f"./.cache/Site_embeddings/{url_name}")
    embedder = OpenAIEmbeddings()
    cache_embedder = CacheBackedEmbeddings.from_bytes_store(embedder, cache_dir)
    vector_store = FAISS.from_documents(docs, cache_embedder)
    return vector_store.as_retriever()
```
#### Chain.py
##### History Model
- `History Model`ì€ userì™€ aiì˜ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ userì˜ ì§ˆë¬¸ì´ ê³¼ê±°ì˜ í–ˆë˜ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ë‚´ìš©ì¸ì§€ë¥¼ íŒë‹¨í•˜ê³  ë¹„ìŠ·í•˜ë‹¤ë©´ ê³¼ê±° ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³  ë¹„ìŠ·í•œ ë‹µë³€ì´ ì—†ë‹¤ë©´ `None`ì„ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •í•œ Modelì´ë‹¤.
- í•´ë‹¹ `prompt`ì— exampleì„ ì œì‹œí•˜ì—¬ ì›í•˜ëŠ” ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ì˜€ë‹¤. 
- userì™€ aiì˜ ëŒ€í™”ë¥¼ `format_message()` functionì„ í†µí•´ exampleì— ë§ê²Œ `ì „ì²˜ë¦¬`í•˜ì˜€ê³ , ë§ˆì§€ë§‰ì— **ìœ ì €ì˜ ì§ˆë¬¸ì´ messageì— í¬í•¨ë˜ì–´ ìˆê¸° ë–„ë¬¸ì— ì´ëŠ” í¬í•¨ë˜ì§€ ì•Šê²Œ ì²˜ë¦¬**í•˜ì˜€ë‹¤. (ì¤‘ë³µ ë‚´ìš© ì œê±°)
- ë¹„ìŠ·í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ê³¼ê±° ê¸°ë¡ì„ ê°€ì ¸ì™€ ê·¸ëŒ€ë¡œ ì¶œë ¥ í•˜ì˜€ì§€ë§Œ ë¹„ìŠ·í•œ ì§ˆë¬¸ì´ ì—†ì„ ì‹œì— ì²˜ìŒì—ëŠ” `None`ì„ ì¶œë ¥í•˜ë‹¤ê°€ ë‹¤ìŒë¶€í„´ `Answer: None`ì„ ì¶œë ¥í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤. ì´ì— `Prompt`ì— `Answer: None`ì„ ì¶œë ¥í•˜ì§€ ë§ë¼ê³  ëª…ì‹œí•˜ì˜€ìœ¼ë‚˜, example ë•Œë¬¸ì¸ì§€ í•´ë‹¹ ë‚´ìš©ì„ ë“£ì§€ ì•Šê³  ê³„ì† `None`ì´ ì•„ë‹Œ `Answer: None`ì„ ì¶œë ¥í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ë‹¤.
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ messageê°€ ì²˜ìŒì¼ ë•ŒëŠ” `History Model`ì„ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ í•˜ì—¬ `None`ì„ ì¶œë ¥ í•˜ì§€ ì•Šê²Œ í•˜ê±°ë‚˜, `Prompt`ì˜ exampleì„ ìˆ˜ì •í•˜ì—¬ `Answer: None`ì„ ì¶œë ¥í•˜ì§€ ì•Šê²Œ í•˜ëŠ” ë“±ì˜ ìˆ˜ì •ì´ í•„ìš”í•  ê²ƒ ê°™ë‹¤.
##### Research Model
- History Modelì—ì„œ ê°’ì´ `None`ì´ ë‚˜ì˜¨ë‹¤ë©´ `Research Model`ì„ ì‹¤í–‰í•˜ì—¬ Retrieverì—ì„œ Userì˜ ì§ˆë¬¸ì— ì•Œë§ì€ ë‹µë³€ì„ ì°¾ì•„ ì¶œë ¥í•´ì£¼ëŠ” Modelì´ë‹¤.
- `Research Model`ì€ `Answers Chain`ê³¼ `Choose Chain`ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ìì„¸í•œ ë‚´ìš©ì€ [[Map Re-rank LCEL Chain]]ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.
- `Choose Chain`ì—ëŠ” [[Memory Modules]](ConversationSummaryBufferMemory) ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•  ë•Œ, ê³¼ê±°ì˜ ë‹µë³€ ë˜í•œ ê³ ë ¤ë˜ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤.
- ê°ê°ì˜ Chainë“¤ì´ `RunnableLambda`ë¡œ ì´ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì•ˆì— ì‹¤í–‰ë˜ëŠ” `function`ì˜Â `Parameter`ì˜ ê²½ìš°ëŠ”Â **`dictionary`Â orÂ `callable object`** ì´ì–´ì•¼ í•œë‹¤ëŠ” ì ì„ ì£¼ì˜í•´ì•¼ í•œë‹¤.
```python
import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationSummaryBufferMemory
from pages.SiteGPT.utils import save_message, send_message

class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.response = ""
        
    def on_llm_start(self, *arg, **kwargs):
        self.message_box = st.empty()
        
    def on_llm_end(self, *arg, **kwargs):
        save_message(st.session_state["messages"], self.response, "ai")
        self.response = ""

    def on_llm_new_token(self, token, *arg, **kwargs):
        self.response += token
        self.message_box.markdown(self.response)

history_prompt = ChatPromptTemplate.from_template(
    """
    You are given a 'history' that the user and ai talked about.

    BASED on this history If a user asks for something SIMILAR, find the answer in history and print it out.

    If the question is not in history, JUST SAY 'None'
    DO NOT SAY 'answer: None' and 'Answer: None'

    examples_1

    History:
    human: What is the color of the occean?
    ai: Blue. Source:https://ko.wikipedia.org/wiki/%EB%B0%94%EB%8B%A4%EC%83%89 Date:2024-10-13

    Question : What color is the ocean?
    Answer : Blue. Source:https://ko.wikipedia.org/wiki/%EB%B0%94%EB%8B%A4%EC%83%89 Date:2024-10-13

    examples_2
    History:
    human: What is the capital of Georgia?
    ai: Tbilisi Source:https://en.wikipedia.org/wiki/Capital_of_Georgia Date:2022-08-22

    Question : What are the major cities in Georgia?
    Answer : Tbilisi Source:https://en.wikipedia.org/wiki/Capital_of_Georgia Date:2022-08-22

    examples_3
    human: When was Avator released?
    ai: 2009 Source:https://en.wikipedia.org/wiki/Avatar_(franchise) Date:2022-12-18

    Question : What is Avator2?
    Answer : None

    examples_4

    History:
    human: What is the capital of the United States?
    ai: Washington, D.C. Source:https://ko.wikipedia.org/wiki/%EB%AF%B8%EA%B5%AD Date:2022-10-18

    Question : What is the capital of the Korea?
    Answer : None

    Your turn!
    History: {history}
    
    Question: {question}
    """
)

  
answers_prompt = ChatPromptTemplate.from_template(
    """
    Using ONLY the following context answer the user's question. If you can't answer,
    Just say you don't know, don't make anyting up.

    Then, give a score to the answer between 0 and 5. 0 being not helpful to
    the user and 5 being helpful to the user.

    Make sure to include the answer's score.
    ONLY one result should be output.

    Context : {context}

    Examples:

    Question: How far away is the moon?
    Answer: The moon is 384,400 km away.
    Score: 5

    Question: How far away is the sun?
    Answer: I don't know
    Score: 0

    Your turn!

    Question : {question}
    """
)

choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Use ONLY the following pre-existing answers to the user's question.

            Use the answers that have the highest score (more helpful) and favor the most recent ones.

            Return the sources of the answers as they are, do not change them.

            You must print out only one answer. and Don't print out the score
            
            Answer: {answers}

            You also have a past answer. Please refert o them and write your answers
            """,
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

history_llm = ChatOpenAI(
    temperature=0.1,
    model="gpt-3.5-turbo-0125",
)

common_llm = ChatOpenAI(
    temperature=0.1,
)

choose_llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationSummaryBufferMemory(
        llm=common_llm,
        memory_key="history",
        max_token_limit=150,
        return_messages=True,
    )

memory = st.session_state["memory"]

def get_answers(inputs):
    docs = inputs["docs"]
    question = inputs["question"]
    answers_chain = answers_prompt | common_llm
    return {
        "question": question,
        "answers": [
            {
                "answer": answers_chain.invoke(
                    {
                        "context": doc.page_content,
                        "question": question,
                    }
                ).content,
                "source": doc.metadata["source"],
                "date": doc.metadata["lastmod"],
            }
            for doc in docs
        ],
        "history": memory.load_memory_variables({})["history"],
    }

def choose_answer(inputs):
    answers = inputs["answers"]
    question = inputs["question"]
    history = inputs["history"]
    choose_chain = choose_prompt | choose_llm
    condensed = "\n\n".join(
        f"{answer['answer']}\nSource:{answer['source']}\nDate:{answer['date']}\n"
        for answer in answers
    )
    return choose_chain.invoke(
        {"question": question, "answers": condensed, "history": history}
    )

def format_message(messages):
    history = ""
    i = 0
    for message in messages:
        if i is not len(messages) - 1:
            history += f"{message['role']} : {message['message']}\n"
        if i % 2 == 1:
            history += "\n"
        i = i + 1
        
    return history

def invoke_chain(messages, retriever, question):  
    history = format_message(messages)
    history_chain = history_prompt | history_llm
    result = history_chain.invoke({"history": history, "question": question})
    response = result.content
    
    if response == "None" or response == "Answer: None":
        research_chain = (
            {
                "docs": retriever,
                "question": RunnablePassthrough(),
            }
            | RunnableLambda(get_answers)
            | RunnableLambda(choose_answer)
        )
        with st.chat_message("ai"):
            answer = research_chain.invoke(question)
            memory.save_context({"input": question}, {"output": answer.content})
    else:
        send_message(messages, response, "ai")
        
def initialize_memory():
    memory.clear()
```