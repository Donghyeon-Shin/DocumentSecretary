# Detail
- [[Video Preprocessing]]ì„ í™œìš©í•˜ì—¬ `Video`ì˜ ì •ë³´ë¥¼ textë¡œ ë³€í™˜í•˜ì—¬ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ videoì„ ìš”ì•½í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì£¼ëŠ” ì¶œë ¥í•´ì£¼ëŠ” Pageì´ë‹¤.
- ê¸°ë³¸ UIëŠ” [[Document GPT]]ì™€ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤.
- [[Streamlit]] `st.bar`ì„ í†µí•´ 3ê°€ì§€ì˜ tabì„ êµ¬ì„±í•˜ì˜€ë‹¤.
	- `transcript_tabs` : textë¡œ ë³€í™˜ëœ `Video`ì˜ ë‚´ìš©ì„ ë³´ì—¬ì£¼ëŠ” `tab`
	- `summary_tabs` : `refine_chain` ì´ìš©í•´ `Video`ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ë³´ì—¬ì£¼ëŠ” `tab`
	- `qa_tab` : `research_chain` ì´ìš©í•´ `Video`ì˜ ë‚´ìš©ì— ëŒ€í•œ `question`ì˜ ë‹µë³€ì„ êµ¬í•´ ë³´ì—¬ì£¼ëŠ” `tab`
- LLMì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.
	- `Refine Model` : [[Refine LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ textë¡œ ë³€í™˜ëœ `Video`ì˜ ë‚´ìš©(`documents`)ì„ ìš”ì•½í•´ì£¼ëŠ” `Model`
	- `Research Model` : [[Map Re-rank LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ Retrieverì—ì„œ Userì˜ `Question`ì— ì•Œë§ì€ ë‹µë³€ì„ ì°¾ì•„ ì¶œë ¥í•´ì£¼ëŠ” `Model`
# Code
- [[Streamlit]]ì˜ `Side bar` Widgitì„ í™œìš©í•˜ì—¬ ìœ ì €ì—ê²Œ `Video`ì„ ë°›ê³  í•´ë‹¹ ê°’ì„ `load` í›„`extract_audio_from_video`, `cut_audio_in_chunks`, `transcribe_chunks` ê³¼ì •ì„ ê±°ì³ `Video`ë¥¼ `text`í™” í•´ íŠ¹ì • ìœ„ì¹˜ì— ì €ì¥í•œë‹¤.
- [[Video Preprocessing]]í•˜ëŠ” ê³¼ì •ì—ì„œ ë¹„ìš©ì´ ë§ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì´ë¯¸ íŠ¹ì • ìœ„ì¹˜ì— ë³€í™˜ëœ `text`ê°€ ì¡´ì¬í•œë‹¤ë©´ ë” ì´ìƒ, ì´ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šë„ë¡ êµ¬í˜„í•˜ì˜€ë‹¤.
- [[Video Preprocessing]]ì—ì„œ  `audio`ë¥¼ ë¶„í• í•  ë•Œë‚˜ `Text_splitter`ì„ ì´ìš©í•´ `text`ì„ ë¶„í• í•  ë•Œ, ë‚´ìš©ì´ ëŠê¸°ê²Œ í•˜ì§€ ì•Šê¸° ìœ„í•´ ì ì ˆí•œ **overlap**ì„ ì„¤ì •í•˜ì—¬ ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.
- `summary_tabs`ì€ ì „ì²´ ë‚´ìš© ëª¨ë‘ ìš”ì•½ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— **ëª¨ë“  ë¬¸ì„œë¥¼ íƒìƒ‰í•˜ì—¬ ë‹µì„ ì •ì œ**í•´ë‚˜ê°€ëŠ” [[Refine LCEL Chain]] ë°©ì‹ì´ ì ì ˆí•˜ë‹¤ ìƒê°í•˜ì—¬, í•´ë‹¹ `Chain`ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
- `qa_tab`ì˜ ê²½ìš°ëŠ” í•´ë‹¹ Pageì—ì„œ ì‚¬ìš©í•œ Videoì˜ ë‚´ìš©ì´ `Bible`ì„ ìš”ì•½ ë° ë‚˜ì—´í•´ì£¼ëŠ” í˜•ì‹ì´ì—ˆê¸° ë•Œë¬¸ì—, **ê°ê°ì˜ ë¶„í• ëœ ë¬¸ì„œì— ë‹µì„ ë§¤ê²¨ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë‹µì„ ì œì‹œ**í•´ì£¼ëŠ” [[Map Re-rank LCEL Chain]] ë°©ì‹ì´ ì ì ˆí•˜ë‹¤ ìƒê°í•˜ì—¬, í•´ë‹¹ `Chain`ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
```python
import streamlit as st
import subprocess
import math
from pydub import AudioSegment
import openai
import glob
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.output_parser import StrOutputParser
from langchain.storage import LocalFileStore
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler

class ChatCallbackHandler(BaseCallbackHandler):
Â  Â  def __init__(self):
Â  Â  Â  Â  self.response = ""
Â  Â  Â  Â  
Â  Â  def on_llm_start(self, *arg, **kwargs):
Â  Â  Â  Â  self.message_box = st.empty()
Â  Â  Â  Â  
Â  Â  def on_llm_new_token(self, token, *arg, **kwargs):
Â  Â  Â  Â  self.response += token
Â  Â  Â  Â  self.message_box.markdown(self.response)
Â  Â  Â  Â  
has_transcrible = os.path.exists("./.cache/meeting_files/chunks/Bible_summary.txt")

llm = ChatOpenAI(
Â  Â  temperature=0.1,
)

choose_llm = ChatOpenAI(
Â  Â  temperature=0.1,
Â  Â  streaming=True,
Â  Â  callbacks=[ChatCallbackHandler()]
)

splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
Â  Â  chunk_size=1000,
Â  Â  chunk_overlap=100,
)

@st.cache_data()
def embed_file(file_path, file_name):
Â  Â  loader = TextLoader(file_path)
Â  Â  documents = loader.load_and_split(text_splitter=splitter)
Â  Â  cache_dir = LocalFileStore(f"./.cache/meeting_files/embeddings/{file_name}")
Â  Â  embedder = OpenAIEmbeddings()
Â  Â  cache_embedder = CacheBackedEmbeddings.from_bytes_store(embedder, cache_dir)
Â  Â  vectorStore = FAISS.from_documents(documents, cache_embedder)
Â  Â  retriever = vectorStore.as_retriever()
Â  Â  return retriever

@st.cache_data()
def transcribe_chunks(chunk_folder, destination):
Â  Â  if has_transcrible:
Â  Â  Â  Â  return
Â  Â  files = glob.glob(f"{chunk_folder}/*.mp3")
Â  Â  files.sort()
Â  Â  for file in files:
Â  Â  Â  Â  with open(file, "rb") as audio_file, open(destination, "a") as text_file:
Â  Â  Â  Â  Â  Â  transcipts = openai.Audio.transcribe(
Â  Â  Â  Â  Â  Â  Â  Â  "whisper-1",
Â  Â  Â  Â  Â  Â  Â  Â  audio_file,
Â  Â  Â  Â  Â  Â  Â  Â  language="ko",
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  text_file.write(transcipts["text"])

@st.cache_data()
def extract_audio_from_video(video_path, audio_path):
Â  Â  if has_transcrible:
Â  Â  Â  Â  return
Â  Â  command = [
Â  Â  Â  Â  "ffmpeg",
Â  Â  Â  Â  "-i",
Â  Â  Â  Â  video_path,
Â  Â  Â  Â  "-vn",
Â  Â  Â  Â  audio_path,
Â  Â  ]
Â  Â  subprocess.run(command)

@st.cache_data()
def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
Â  Â  if has_transcrible:
Â  Â  Â  Â  return
Â  Â  track = AudioSegment.from_mp3(audio_path)
Â  Â  chunk_overlap = 10 * 1000 Â # overlap_size = 10 seconds
Â  Â  chunk_len = chunk_size * 60 * 1000 - chunk_overlap
Â  Â  chunks = math.ceil(len(track) / chunk_len)
Â  Â  for i in range(chunks):
Â  Â  Â  Â  start_time = i * chunk_len
Â  Â  Â  Â  end_time = (i + 1) * chunk_len + chunk_overlap
Â  Â  Â  Â  chunk = track[start_time:end_time]
Â  Â  Â  Â  chunk.export(f"./{chunks_folder}/chunk_{i}.mp3", format="mp3")

def get_answers(inputs):
Â  Â  docs = inputs["docs"]
Â  Â  question = inputs["question"]
Â  Â  answer_prompt = ChatPromptTemplate.from_template(
Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Using ONLY the following context answer the user's question. If you can't answer,
Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  JUST say you don't know. don't make anything up.

Â  Â  Â  Â  Â  Â  Then, give a score to the answer between 0 and 5. 0 being not helpful to the user and 5 being helpful to the user.

Â  Â  Â  Â  Â  Â  Make sure to include the answer's score.

Â  Â  Â  Â  Â  Â  ONLY one result should be output.
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Content : {context}

Â  Â  Â  Â  Â  Â  Examples:

Â  Â  Â  Â  Â  Â  Question: How far away the moon?
Â  Â  Â  Â  Â  Â  Answer: The moon is 384,400 km away.
Â  Â  Â  Â  Â  Â  Score: 5

Â  Â  Â  Â  Â  Â  Question: How far away is the sun?
Â  Â  Â  Â  Â  Â  Answer: I don't know
Â  Â  Â  Â  Â  Â  Score: 0

Â  Â  Â  Â  Â  Â  Your turn!
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Question : {question}
Â  Â  Â  Â  Â  Â  """
Â  Â  )

Â  Â  answer_chain = answer_prompt | llm | StrOutputParser()
Â  Â  
Â  Â  return {
Â  Â  Â  Â  "question": question,
Â  Â  Â  Â  "answers": [
Â  Â  Â  Â  Â  Â  answer_chain.invoke(
Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "context": doc.page_content,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "question": question,
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  for doc in docs
Â  Â  Â  Â  ],
Â  Â  }

def choose_answer(inputs):
Â  Â  question = inputs["question"]
Â  Â  answers = inputs["answers"]
Â  Â  format_answers = "\n\n".join(answer for answer in answers)
Â  Â  choose_prompt = ChatPromptTemplate.from_messages(
Â  Â  Â  Â  [
Â  Â  Â  Â  Â  Â  (
Â  Â  Â  Â  Â  Â  Â  Â  "system",
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Use ONLY the following pre-existing answers to the user's question.

Â  Â  Â  Â  Â  Â  Â  Â  Use the answers that have the highest score (more helpful) and favor the most recent ones.

Â  Â  Â  Â  Â  Â  Â  Â  Return the sources of the answers as they are, do not change them.

Â  Â  Â  Â  Â  Â  Â  Â  You must print out only one answer. and Don't print out the score

Â  Â  Â  Â  Â  Â  Â  Â  Answer: {answers}
Â  Â  Â  Â  Â  Â  Â  Â  """,
Â  Â  Â  Â  Â  Â  ),
Â  Â  Â  Â  Â  Â  ("human", "{question}"),
Â  Â  Â  Â  ]
Â  Â  )

Â  Â  choose_chain = choose_prompt | choose_llm | StrOutputParser()

Â  Â  respose = choose_chain.invoke({"answers" : format_answers, "question" : question})
Â  Â  return respose

st.set_page_config(
Â  Â  page_title="MettingGPT",
Â  Â  page_icon="ğŸ¤£",
)

st.title("MeetingGPT")

st.markdown(
Â  Â  """
Â  Â  Welcome to MettingGPT, upload a video and I will give you a transcript, a summary and a chat bot to ask any question about it.

Â  Â  Get started by uploading a video file in the sidebar.
Â  Â  """
)

with st.sidebar:
Â  Â  video = st.file_uploader("Video", type=["mp4", "avi", "mkv", "mov"])
if video:
Â  Â  with st.status("Loading video....") as status:
Â  Â  Â  Â  video_content = video.read()
Â  Â  Â  Â  video_path = f"./.cache/meeting_files/{video.name}"
Â  Â  Â  Â  audio_path = video_path.replace("mp4", "mp3")
Â  Â  Â  Â  chuck_path = "./.cache/meeting_files/chunks"
Â  Â  Â  Â  transcribe_path = video_path.replace("mp4", "txt")
Â  Â  Â  Â  with open(video_path, "wb") as f:
Â  Â  Â  Â  Â  Â  f.write(video_content)
Â  Â  Â  Â  status.update(label="Extracting audio....")
Â  Â  Â  Â  extract_audio_from_video(video_path, audio_path)
Â  Â  Â  Â  status.update(label="Cutting audio segments....")
Â  Â  Â  Â  cut_audio_in_chunks(audio_path, 10, chuck_path)
Â  Â  Â  Â  status.update(label="Transcribing audio....")
Â  Â  Â  Â  transcribe_chunks(chuck_path, transcribe_path)

Â  Â  transcript_tabs, summary_tabs, qa_tab = st.tabs(
Â  Â  Â  Â  [
Â  Â  Â  Â  Â  Â  "Transcript",
Â  Â  Â  Â  Â  Â  "Summary",
Â  Â  Â  Â  Â  Â  "Q&A",
Â  Â  Â  Â  ]
Â  Â  )

Â  Â  with transcript_tabs:
Â  Â  Â  Â  with open(transcribe_path, "r") as file:
Â  Â  Â  Â  Â  Â  st.write(file.read())
Â  Â  Â  Â  Â  Â  
Â  Â  with summary_tabs:
Â  Â  Â  Â  summary_start_button = st.button("Generate Summary")
Â  Â  Â  Â  if summary_start_button:
Â  Â  Â  Â  Â  Â  loader_path = "./.cache/meeting_files/Bible_small_summary.txt"
Â  Â  Â  Â  Â  Â  loader = TextLoader(loader_path)
Â  Â  Â  Â  Â  Â  docs = loader.load_and_split(text_splitter=splitter)
Â  Â  Â  Â  Â  Â  first_summary_prompt = ChatPromptTemplate.from_template(
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Write a concise summary of the following:
Â  Â  Â  Â  Â  Â  Â  Â  "{text}"
Â  Â  Â  Â  Â  Â  Â  Â  CONCISE SUMMARY:
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  first_summary_chain = first_summary_prompt | llm | StrOutputParser()
Â  Â  Â  Â  Â  Â  summary = first_summary_chain.invoke({"text": docs[0].page_content})
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  refine_prompt = ChatPromptTemplate.from_template(
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Your job is to produce a final summary.
Â  Â  Â  Â  Â  Â  Â  Â  We have provided an existing summary up to a certain point: {existing_summary}
Â  Â  Â  Â  Â  Â  Â  Â  We have the opportunity to refine the existing summary (only if needed) with some more context below.
Â  Â  Â  Â  Â  Â  Â  Â  ---------
Â  Â  Â  Â  Â  Â  Â  Â  {context}
Â  Â  Â  Â  Â  Â  Â  Â  ---------
Â  Â  Â  Â  Â  Â  Â  Â  Given the new context, refine the original summary.
Â  Â  Â  Â  Â  Â  Â  Â  If the context isn't useful, RETURN the original summary.
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  refine_chain = refine_prompt | llm | StrOutputParser()

Â  Â  Â  Â  Â  Â  with st.status("Summarizing") as status:
Â  Â  Â  Â  Â  Â  Â  Â  for i, doc in enumerate(docs[1:]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status.update(label=f"Processing document {i+1}/{len(docs)-1}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary = refine_chain.invoke(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "existing_summary": summary,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "context": doc.page_content,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.write(summary)
Â  Â  Â  Â  Â  Â  st.write(summary)

Â  Â  with qa_tab:
Â  Â  Â  Â  retriever = embed_file(transcribe_path, video.name)
Â  Â  Â  Â  docs = retriever.invoke("Dose he talk about the bible?")
Â  Â  Â  Â  question = st.text_input("Answer anyting about the video")
Â  Â  Â  Â  if question:
Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  research_chain = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "docs": retriever,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "question": RunnablePassthrough(),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  | RunnableLambda(get_answers)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  | RunnableLambda(choose_answer)
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  research_chain.invoke(question)
```