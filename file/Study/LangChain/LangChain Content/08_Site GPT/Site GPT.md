# Detail
- [[Site Loader]]ë¥¼ í™œìš©í•˜ì—¬ `Site`ì˜ ì •ë³´ë¥¼ ê°€ì €ì™€ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì£¼ëŠ” ì¶œë ¥í•´ì£¼ëŠ” Pageì´ë‹¤.
- ê¸°ë³¸ UIëŠ” [[Document GPT]]ì™€ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤.
- LLMì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.
	- `History Model` : [[Stuff LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ Userì˜ `Question`ê³¼ `Histroy`(AIì™€ userì˜ Message)ë¥¼ ë°›ì€ ë’¤, í•´ë‹¹ `Question`ì´ ì˜ˆì „ì˜ ì¡´ì¬í•˜ì˜€ë‹¤ë©´ í•´ë‹¹ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³  ì¡´ì¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´ `None`ì„ ì¶œë ¥í•´ì£¼ëŠ” `Model`
	- `Research Model` : [[Map Re-rank LCEL Chain]]ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ Retrieverì—ì„œ Userì˜ `Question`ì— ì•Œë§ì€ ë‹µë³€ì„ ì°¾ì•„ ì¶œë ¥í•´ì£¼ëŠ” `Model`
- `Memory`, `Message`, `Chain` ë“± ì—¬ëŸ¬ ê¸°ëŠ¥ë“¤ì„ ì‰½ê²Œ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê°ê°ì„ **package ë¶„í• **í•˜ì˜€ë‹¤.
	- `SiteGPT.py` : `SiteGPT`ì˜ ë©”ì¸ Pageë¥¼ UIì„ êµ¬ì„±í•˜ì—¬ ì¶œë ¥í•˜ëŠ” file
	- `Utils.py` : Userì™€ AIì˜ ëŒ€í™”ë¥¼ ì¶œë ¥í•˜ê³  ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
	- `Data_process.py` :  [[Site Loader]]ë¡œ ë°ì´í„°ë¥¼ ë°›ê±°ë‚˜ í•´ë‹¹ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
	- `Chain.py` : `LLM`, `Memory` ê´€ë ¨ëœ ëª¨ë“  ê¸°ëŠ¥ë“¤ì˜ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ package
# Code
#### SiteGPT.py
- [[Streamlit]]ì˜ `Side bar` Widgitì„ í™œìš©í•˜ì—¬ ìœ ì €ì—ê²Œ `URL`ì„ ë°›ê³  í•´ë‹¹ ê°’ì„ `Data_process.py`ì— ë„˜ê²¨ Retreiverì„ ë°›ëŠ”ë‹¤.
- Retreiver ê°’ì— ìœ ì €ì˜ ì§ˆë¬¸ì„ ë”í•˜ì—¬ `Chain.py` ë„˜ê²¨ `AI`ì˜ Responseì„ ë„˜ê²¨ ë°›ëŠ”ë‹¤. 
- ì´ëŸ¬í•œ ê³¼ì •ì„ ê±°ì³ ì–»ì€ Responseì„ `Utils.py`ì˜ functionì„ ì´ìš©í•´ Siteì— ì¶œë ¥í•œë‹¤.
- URLì˜ ê°’ì´ ì—†ì„ ë•Œ, `Memory` ê°’ê³¼ `Message`ì´ ì´ˆê¸°í™” ë˜ë„ë¡ êµ¬í˜„í•˜ì˜€ë‹¤.
```python
import streamlit as st
from pages.SiteGPT.utils import paint_message, send_message
from pages.SiteGPT.data_process import get_retriever_in_website
from pages.SiteGPT.chain import invoke_chain, initialize_memory

st.set_page_config(
Â  Â  page_title="Site GPT",
Â  Â  page_icon="ğŸ¤£",
)

st.title("Site GPT")

st.markdown(
Â  Â  """
Â  Â  Ask questions about the content of a website.

Â  Â  Start by writing the URL of the website on the sidebar.
Â  Â  """
)

# ex) https://deepmind.google/sitemap.xml

with st.sidebar:
Â  Â  url = st.text_input(
Â  Â  Â  Â  "Write down a URL",
Â  Â  Â  Â  placeholder="https://example.com/sitemap.xml",
Â  Â  )

if url:
Â  Â  if ".xml" not in url:
Â  Â  Â  Â  with st.sidebar:
Â  Â  Â  Â  Â  Â  st.error("Please write down a Stiemap URL")
Â  Â  else:
Â  Â  Â  Â  retriever = get_retriever_in_website(url)
Â  Â  Â  Â  send_message(st.session_state["messages"], "How can I help you?", "ai", save=False)
Â  Â  Â  Â  paint_message(st.session_state["messages"])
Â  Â  Â  Â  question = st.chat_input("Ask any questions in the document!")
Â  Â  Â  Â  if question:
Â  Â  Â  Â  Â  Â  send_message(st.session_state["messages"], question, "human")
Â  Â  Â  Â  Â  Â  invoke_chain(st.session_state["messages"], retriever, question)
else:
Â  Â  st.session_state["messages"] = []
Â  Â  initialize_memory()
```
#### Utils.py
- `paint_message` : ê¸°ë¡ëœ ëª¨ë“  messagesì„ ì¶œë ¥í•œë‹¤.
- `save_message` : messageì™€ roleë¥¼ ì €ì¥í•œë‹¤.
- `send_message` : messageì„ ì¶œë ¥í•˜ê³  `Save` ì—¬ë¶€ì— ë”°ë¼ messageë¥¼ ì €ì¥í•œë‹¤.
```python
import streamlit as st

def paint_message(messages):
Â  Â  for message in messages:
Â  Â  Â  Â  send_message(messages, message["message"], message["role"], save=False)
Â  Â  Â  Â  
def save_message(messages, message, role):
Â  Â  messages.append({"message": message, "role": role})

def send_message(messages, message, role, save=True):
Â  Â  with st.chat_message(role):
Â  Â  Â  Â  st.markdown(message)
Â  Â  Â  Â  if save:
Â  Â  Â  Â  Â  Â  save_message(messages, message, role)
```
#### Data_process.py
- `parse_page` : `SitemapLoader`ì„ í†µí•´ ê°€ì ¸ì˜¨ `Data`ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.
- `get_retriever_in_website` : `st.cache_resource`ì„ ì‚¬ìš©í•˜ì—¬ URLì´ ë°”ë€” ë•Œë§Œ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ê³ , [[Retrieval]]ì˜ ì „ë°˜ì ì¸ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.
```python
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import SitemapLoader
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings

def parse_page(soup):
Â  Â  header = soup.find("header")
Â  Â  footer = soup.find("footer")
Â  Â  if header:
Â  Â  Â  Â  header.decompose()
Â  Â  if footer:
Â  Â  Â  Â  footer.decompose()
Â  Â  return str(soup.get_text()).replace("\n", " ").replace("\xa0", " ")

@st.cache_resource(show_spinner="Loading website....")
def get_retriever_in_website(url):
Â  Â  splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
Â  Â  Â  Â  chunk_size=1000,
Â  Â  Â  Â  chunk_overlap=200,
Â  Â  )
Â  Â  loader = SitemapLoader(
Â  Â  Â  Â  url,
Â  Â  Â  Â  parsing_function=parse_page,
Â  Â  )
Â  Â  loader.requests_per_second = 5
Â  Â  docs = loader.load_and_split(text_splitter=splitter)
Â  Â  url_name = (
Â  Â  Â  Â  str(url).replace("https://", "").replace(".", "").replace("/sitemapxml", "")
Â  Â  )
Â  Â  cache_dir = LocalFileStore(f"./.cache/Site_embeddings/{url_name}")
Â  Â  embedder = OpenAIEmbeddings()
Â  Â  cache_embedder = CacheBackedEmbeddings.from_bytes_store(embedder, cache_dir)
Â  Â  vector_store = FAISS.from_documents(docs, cache_embedder)
Â  Â  return vector_store.as_retriever()
```
#### Chain.py
##### History Model
- `History Model`ì€ userì™€ aiì˜ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ userì˜ ì§ˆë¬¸ì´ ê³¼ê±°ì˜ í–ˆë˜ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ë‚´ìš©ì¸ì§€ë¥¼ íŒë‹¨í•˜ê³  ë¹„ìŠ·í•˜ë‹¤ë©´ ê³¼ê±° ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³  ë¹„ìŠ·í•œ ë‹µë³€ì´ ì—†ë‹¤ë©´ `None`ì„ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •í•œ Modelì´ë‹¤.
- í•´ë‹¹ `prompt`ì— exampleì„ ì œì‹œí•˜ì—¬ ì›í•˜ëŠ” ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ì˜€ë‹¤. 
- userì™€ aiì˜ ëŒ€í™”ë¥¼ `format_message()` functionì„ í†µí•´ exampleì— ë§ê²Œ `ì „ì²˜ë¦¬`í•˜ì˜€ê³ , ë§ˆì§€ë§‰ì— **ìœ ì €ì˜ ì§ˆë¬¸ì´ messageì— í¬í•¨ë˜ì–´ ìˆê¸° ë–„ë¬¸ì— ì´ëŠ” í¬í•¨ë˜ì§€ ì•Šê²Œ ì²˜ë¦¬**í•˜ì˜€ë‹¤. (ì¤‘ë³µ ë‚´ìš© ì œê±°)
- ë¹„ìŠ·í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ê³¼ê±° ê¸°ë¡ì„ ê°€ì ¸ì™€ ê·¸ëŒ€ë¡œ ì¶œë ¥ í•˜ì˜€ì§€ë§Œ ë¹„ìŠ·í•œ ì§ˆë¬¸ì´ ì—†ì„ ì‹œì— ì²˜ìŒì—ëŠ” `None`ì„ ì¶œë ¥í•˜ë‹¤ê°€ ë‹¤ìŒë¶€í„´ `Answer: None`ì„ ì¶œë ¥í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤. ì´ì— `Prompt`ì— `Answer: None`ì„ ì¶œë ¥í•˜ì§€ ë§ë¼ê³  ëª…ì‹œí•˜ì˜€ìœ¼ë‚˜, example ë•Œë¬¸ì¸ì§€ í•´ë‹¹ ë‚´ìš©ì„ ë“£ì§€ ì•Šê³  ê³„ì† `None`ì´ ì•„ë‹Œ `Answer: None`ì„ ì¶œë ¥í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ë‹¤.
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ messageê°€ ì²˜ìŒì¼ ë•ŒëŠ” `History Model`ì„ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ í•˜ì—¬ `None`ì„ ì¶œë ¥ í•˜ì§€ ì•Šê²Œ í•˜ê±°ë‚˜, `Prompt`ì˜ exampleì„ ìˆ˜ì •í•˜ì—¬ `Answer: None`ì„ ì¶œë ¥í•˜ì§€ ì•Šê²Œ í•˜ëŠ” ë“±ì˜ ìˆ˜ì •ì´ í•„ìš”í•  ê²ƒ ê°™ë‹¤.
##### Research Model
- History Modelì—ì„œ ê°’ì´ `None`ì´ ë‚˜ì˜¨ë‹¤ë©´ `Research Model`ì„ ì‹¤í–‰í•˜ì—¬ Retrieverì—ì„œ Userì˜ ì§ˆë¬¸ì— ì•Œë§ì€ ë‹µë³€ì„ ì°¾ì•„ ì¶œë ¥í•´ì£¼ëŠ” Modelì´ë‹¤.
- `Research Model`ì€ `Answers Chain`ê³¼ `Choose Chain`ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ìì„¸í•œ ë‚´ìš©ì€ [[Map Re-rank LCEL Chain]]ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.
- `Choose Chain`ì—ëŠ” [[Memory Modules]](ConversationSummaryBufferMemory) ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•  ë•Œ, ê³¼ê±°ì˜ ë‹µë³€ ë˜í•œ ê³ ë ¤ë˜ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤.
- ê°ê°ì˜ Chainë“¤ì´ `RunnableLambda`ë¡œ ì´ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì•ˆì— ì‹¤í–‰ë˜ëŠ” `function`ì˜Â `Parameter`ì˜ ê²½ìš°ëŠ”Â **`dictionary`Â orÂ `callable object`** ì´ì–´ì•¼ í•œë‹¤ëŠ” ì ì„ ì£¼ì˜í•´ì•¼ í•œë‹¤.
```python
import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationSummaryBufferMemory
from pages.SiteGPT.utils import save_message, send_message

class ChatCallbackHandler(BaseCallbackHandler):
Â  Â  def __init__(self):
Â  Â  Â  Â  self.response = ""
Â  Â  Â  Â  
Â  Â  def on_llm_start(self, *arg, **kwargs):
Â  Â  Â  Â  self.message_box = st.empty()
Â  Â  Â  Â  
Â  Â  def on_llm_end(self, *arg, **kwargs):
Â  Â  Â  Â  save_message(st.session_state["messages"], self.response, "ai")
Â  Â  Â  Â  self.response = ""

Â  Â  def on_llm_new_token(self, token, *arg, **kwargs):
Â  Â  Â  Â  self.response += token
Â  Â  Â  Â  self.message_box.markdown(self.response)

history_prompt = ChatPromptTemplate.from_template(
Â  Â  """
Â  Â  You are given a 'history' that the user and ai talked about.

Â  Â  BASED on this history If a user asks for something SIMILAR, find the answer in history and print it out.

Â  Â  If the question is not in history, JUST SAY 'None'
Â  Â  DO NOT SAY 'answer: None' and 'Answer: None'

Â  Â  examples_1

Â  Â  History:
Â  Â  human: What is the color of the occean?
Â  Â  ai: Blue. Source:https://ko.wikipedia.org/wiki/%EB%B0%94%EB%8B%A4%EC%83%89 Date:2024-10-13

Â  Â  Question : What color is the ocean?
Â  Â  Answer : Blue. Source:https://ko.wikipedia.org/wiki/%EB%B0%94%EB%8B%A4%EC%83%89 Date:2024-10-13

Â  Â  examples_2
Â  Â  History:
Â  Â  human: What is the capital of Georgia?
Â  Â  ai: Tbilisi Source:https://en.wikipedia.org/wiki/Capital_of_Georgia Date:2022-08-22

Â  Â  Question : What are the major cities in Georgia?
Â  Â  Answer : Tbilisi Source:https://en.wikipedia.org/wiki/Capital_of_Georgia Date:2022-08-22

Â  Â  examples_3
Â  Â  human: When was Avator released?
Â  Â  ai: 2009 Source:https://en.wikipedia.org/wiki/Avatar_(franchise) Date:2022-12-18

Â  Â  Question : What is Avator2?
Â  Â  Answer : None

Â  Â  examples_4

Â  Â  History:
Â  Â  human: What is the capital of the United States?
Â  Â  ai: Washington, D.C. Source:https://ko.wikipedia.org/wiki/%EB%AF%B8%EA%B5%AD Date:2022-10-18

Â  Â  Question : What is the capital of the Korea?
Â  Â  Answer : None

Â  Â  Your turn!
Â  Â  History: {history}
Â  Â  
Â  Â  Question: {question}
Â  Â  """
)

  
answers_prompt = ChatPromptTemplate.from_template(
Â  Â  """
Â  Â  Using ONLY the following context answer the user's question. If you can't answer,
Â  Â  Just say you don't know, don't make anyting up.

Â  Â  Then, give a score to the answer between 0 and 5. 0 being not helpful to
Â  Â  the user and 5 being helpful to the user.

Â  Â  Make sure to include the answer's score.
Â  Â  ONLY one result should be output.

Â  Â  Context : {context}

Â  Â  Examples:

Â  Â  Question: How far away is the moon?
Â  Â  Answer: The moon is 384,400 km away.
Â  Â  Score: 5

Â  Â  Question: How far away is the sun?
Â  Â  Answer: I don't know
Â  Â  Score: 0

Â  Â  Your turn!

Â  Â  Question : {question}
Â  Â  """
)

choose_prompt = ChatPromptTemplate.from_messages(
Â  Â  [
Â  Â  Â  Â  (
Â  Â  Â  Â  Â  Â  "system",
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Use ONLY the following pre-existing answers to the user's question.

Â  Â  Â  Â  Â  Â  Use the answers that have the highest score (more helpful) and favor the most recent ones.

Â  Â  Â  Â  Â  Â  Return the sources of the answers as they are, do not change them.

Â  Â  Â  Â  Â  Â  You must print out only one answer. and Don't print out the score
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Answer: {answers}

Â  Â  Â  Â  Â  Â  You also have a past answer. Please refert o them and write your answers
Â  Â  Â  Â  Â  Â  """,
Â  Â  Â  Â  ),
Â  Â  Â  Â  MessagesPlaceholder(variable_name="history"),
Â  Â  Â  Â  ("human", "{question}"),
Â  Â  ]
)

history_llm = ChatOpenAI(
Â  Â  temperature=0.1,
Â  Â  model="gpt-3.5-turbo-0125",
)

common_llm = ChatOpenAI(
Â  Â  temperature=0.1,
)

choose_llm = ChatOpenAI(
Â  Â  temperature=0.1,
Â  Â  streaming=True,
Â  Â  callbacks=[ChatCallbackHandler()],
)

if "memory" not in st.session_state:
Â  Â  st.session_state["memory"] = ConversationSummaryBufferMemory(
Â  Â  Â  Â  llm=common_llm,
Â  Â  Â  Â  memory_key="history",
Â  Â  Â  Â  max_token_limit=150,
Â  Â  Â  Â  return_messages=True,
Â  Â  )

memory = st.session_state["memory"]

def get_answers(inputs):
Â  Â  docs = inputs["docs"]
Â  Â  question = inputs["question"]
Â  Â  answers_chain = answers_prompt | common_llm
Â  Â  return {
Â  Â  Â  Â  "question": question,
Â  Â  Â  Â  "answers": [
Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  "answer": answers_chain.invoke(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "context": doc.page_content,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "question": question,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  ).content,
Â  Â  Â  Â  Â  Â  Â  Â  "source": doc.metadata["source"],
Â  Â  Â  Â  Â  Â  Â  Â  "date": doc.metadata["lastmod"],
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  for doc in docs
Â  Â  Â  Â  ],
Â  Â  Â  Â  "history": memory.load_memory_variables({})["history"],
Â  Â  }

def choose_answer(inputs):
Â  Â  answers = inputs["answers"]
Â  Â  question = inputs["question"]
Â  Â  history = inputs["history"]
Â  Â  choose_chain = choose_prompt | choose_llm
Â  Â  condensed = "\n\n".join(
Â  Â  Â  Â  f"{answer['answer']}\nSource:{answer['source']}\nDate:{answer['date']}\n"
Â  Â  Â  Â  for answer in answers
Â  Â  )
Â  Â  return choose_chain.invoke(
Â  Â  Â  Â  {"question": question, "answers": condensed, "history": history}
Â  Â  )

def format_message(messages):
Â  Â  history = ""
Â  Â  i = 0
Â  Â  for message in messages:
Â  Â  Â  Â  if i is not len(messages) - 1:
Â  Â  Â  Â  Â  Â  history += f"{message['role']} : {message['message']}\n"
Â  Â  Â  Â  if i % 2 == 1:
Â  Â  Â  Â  Â  Â  history += "\n"
Â  Â  Â  Â  i = i + 1
Â  Â  Â  Â  
Â  Â  return history

def invoke_chain(messages, retriever, question):  
Â  Â  history = format_message(messages)
Â  Â  history_chain = history_prompt | history_llm
Â  Â  result = history_chain.invoke({"history": history, "question": question})
Â  Â  response = result.content
Â  Â  
Â  Â  if response == "None" or response == "Answer: None":
Â  Â  Â  Â  research_chain = (
Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  "docs": retriever,
Â  Â  Â  Â  Â  Â  Â  Â  "question": RunnablePassthrough(),
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  | RunnableLambda(get_answers)
Â  Â  Â  Â  Â  Â  | RunnableLambda(choose_answer)
Â  Â  Â  Â  )
Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  answer = research_chain.invoke(question)
Â  Â  Â  Â  Â  Â  memory.save_context({"input": question}, {"output": answer.content})
Â  Â  else:
Â  Â  Â  Â  send_message(messages, response, "ai")
Â  Â  Â  Â  
def initialize_memory():
Â  Â  memory.clear()
```